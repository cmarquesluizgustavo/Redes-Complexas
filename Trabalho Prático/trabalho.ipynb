{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_metrics(G: nx.Graph) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate the distance metrics of the graph G.\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for node in G.nodes():\n",
    "        this_node_to_all = nx.shortest_path_length(G, source=node)\n",
    "        for n, d in this_node_to_all.items():\n",
    "            if d != 0:\n",
    "                distances.append(d)\n",
    "    return distances\n",
    "\n",
    "def CCDF(G: nx.Graph) -> list:\n",
    "    \"\"\"\n",
    "    Calculate the CCDF of the graph G.\n",
    "    \"\"\"\n",
    "    degree_sequence = sorted([d for n, d in G.degree()], reverse=True)\n",
    "    degreeCount = np.array(np.unique(degree_sequence, return_counts=True)).T\n",
    "    ccdf = np.array([[degreeCount[i,0], np.sum(degreeCount[i:,1])] for i in range(len(degreeCount))])\n",
    "    return ccdf\n",
    "\n",
    "def generate_metrics_statistics(metric_name: str, data_list: list) -> str:\n",
    "    \"\"\"\n",
    "    Generate the statistics of metric in the graph. Return in a string format.\n",
    "    \"\"\"\n",
    "    result = f\"### {metric_name} Analysis ###\\n\"\n",
    "    result += f\"Average: {np.mean(data_list)}\\n\"\n",
    "    result += f\"Median: {np.median(data_list)}\\n\"\n",
    "    result += f\"Standard Deviation: {np.std(data_list)}\\n\"\n",
    "    result += f\"Variance: {np.var(data_list)}\\n\"\n",
    "    result += f\"Max: {np.max(data_list)}\\n\"\n",
    "    result += f\"Min: {np.min(data_list)}\\n\"\n",
    "    result += f\"CCDF: {np.sum(data_list)}\\n\"\n",
    "    return result\n",
    "\n",
    "def generate_graph_metrics(G: nx.Graph) -> dict:\n",
    "    \"\"\"\n",
    "    Generate the metrics of the graph G.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    result[\"Degree\"] = generate_metrics_statistics(\"Degree\", list(dict(G.degree()).values()))\n",
    "    result[\"Distances\"] = generate_metrics_statistics(\"Clustering\", calculate_distance_metrics(G))\n",
    "    result[\"Betweenness\"] = generate_metrics_statistics(\"Betweenness\", list(nx.betweenness_centrality(G).values()))\n",
    "    result[\"Closeness\"] = generate_metrics_statistics(\"Closeness\", list(nx.closeness_centrality(G).values()))\n",
    "    result[\"Eigenvector\"] = generate_metrics_statistics(\"Eigenvector\", list(nx.eigenvector_centrality(G).values()))\n",
    "    result[\"PageRank\"] = generate_metrics_statistics(\"PageRank\", list(nx.pagerank(G).values()))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all as data\n",
    "files = os.listdir('data/as')\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# read the graph\n",
    "len_files, i = len(files), 0\n",
    "for file in files:\n",
    "    print('Reading file: ', i, 'Out of ', len_files)\n",
    "    G.add_edges_from(nx.read_edgelist('data/as/' + file, create_using=nx.DiGraph(), nodetype=int).edges())\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree analysis\n",
      "Max:  3704\n",
      "Min:  2\n",
      "Avg:  11.831259720062208\n",
      "Std:  63.49473339592536\n",
      "Median:  6.0\n"
     ]
    }
   ],
   "source": [
    "# Degree analysis. max, min, avg, std, median, CCDF\n",
    "degree = [d for n, d in G.degree()]\n",
    "print('Degree analysis')\n",
    "print('Max: ', max(degree))\n",
    "print('Min: ', min(degree))\n",
    "print('Avg: ', sum(degree) / len(degree))\n",
    "print('Std: ', np.std(degree))\n",
    "print('Median: ', np.median(degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance analysis\n",
      "Max:  8\n",
      "Min:  1\n",
      "Avg:  3.4712252225556175\n",
      "Std:  0.8476635946952388\n",
      "Median:  3.0\n"
     ]
    }
   ],
   "source": [
    "# Distance analysis. max, min, avg, std, median, CCDF\n",
    "distances = []\n",
    "for node in G.nodes():\n",
    "    this_node_to_all = nx.shortest_path_length(G, source=node)\n",
    "    for n, d in this_node_to_all.items():\n",
    "        if d != 0:\n",
    "            distances.append(d)\n",
    "print('Distance analysis')\n",
    "print('Max: ', max(distances))\n",
    "print('Min: ', min(distances))\n",
    "print('Avg: ', sum(distances) / len(distances))\n",
    "print('Std: ', np.std(distances))\n",
    "print('Median: ', np.median(distances))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of connected components analysis\n",
      "Max:  7716\n",
      "Min:  7716\n",
      "Avg:  7716.0\n",
      "Std:  0.0\n",
      "Median:  7716.0\n"
     ]
    }
   ],
   "source": [
    "# Size of connected components analysis. max, min, avg, std, median, CCDF\n",
    "components = [len(c) for c in sorted(nx.strongly_connected_components(G), key=len, reverse=True)]\n",
    "print('Size of connected components analysis')\n",
    "print('Max: ', max(components))\n",
    "print('Min: ', min(components))\n",
    "print('Avg: ', sum(components) / len(components))\n",
    "print('Std: ', np.std(components))\n",
    "print('Median: ', np.median(components))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering coefficient analysis\n",
      "Max:  1.0\n",
      "Min:  0\n",
      "Avg:  0.3915524437675338\n",
      "Std:  0.41536848103878343\n",
      "Median:  0.2559602302459445\n"
     ]
    }
   ],
   "source": [
    "# Clustering coefficient analysis. max, min, avg, std, median, CCDF\n",
    "clustering = nx.clustering(G)\n",
    "print('Clustering coefficient analysis')\n",
    "print('Max: ', max(clustering.values()))\n",
    "print('Min: ', min(clustering.values()))\n",
    "print('Avg: ', sum(clustering.values()) / len(clustering.values()))\n",
    "print('Std: ', np.std(list(clustering.values())))\n",
    "print('Median: ', np.median(list(clustering.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betweenness centrality analysis\n",
      "Max:  0.31277529553324546\n",
      "Min:  0.0\n",
      "Avg:  0.0003203558753637029\n",
      "Std:  0.004862849332956846\n",
      "Median:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Betweenness centrality analysis. max, min, avg, std, median, CCDF\n",
    "betweenness = nx.betweenness_centrality(G)\n",
    "print('Betweenness centrality analysis')\n",
    "print('Max: ', max(betweenness.values()))\n",
    "print('Min: ', min(betweenness.values()))\n",
    "print('Avg: ', sum(betweenness.values()) / len(betweenness.values()))\n",
    "print('Std: ', np.std(list(betweenness.values())))\n",
    "print('Median: ', np.median(list(betweenness.values())))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
